import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import json
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
import pandas as pd
from langsmith import Client
from .evaluate import *
from .evaluate_llm import *
import pymongo

def display_constraints_eval_result(agent_data=None):
    #st.set_page_config(layout="wide")
    st.title("ğŸµ LLM ê¸°ë°˜ ìŒì•… ì¶”ì²œ ì„œë¹„ìŠ¤ ì œì•½ ì¡°ê±´ ì¤€ìˆ˜ í‰ê°€")

    st.markdown("""
    ì´ ëŒ€ì‹œë³´ë“œëŠ” LLM ê¸°ë°˜ ìŒì•… ì¶”ì²œ ì„œë¹„ìŠ¤ì˜ íŠ¹ì • ì œì•½ ì¡°ê±´ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì—ì´ì „íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ 'ì‚¬ìš©ì ì„ í˜¸ë„ ë¯¸ë°˜ì˜' ë° 'ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œ ê¸ˆì§€'ì™€ ê°™ì€
    í•µì‹¬ ì œì•½ ì¡°ê±´ì´ ì–¼ë§ˆë‚˜ ì˜ ì§€ì¼œì¡ŒëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """)

    st.markdown(
    """
    <style>
    div[data-testid="stMetricValue"] {
        font-size: 24px; /* ê°’(ìˆ«ì)ì˜ í°íŠ¸ í¬ê¸° */
    }
    div[data-testid="stMetricLabel"] > div {
        font-size: 14px; /* ë¼ë²¨(í…ìŠ¤íŠ¸)ì˜ í°íŠ¸ í¬ê¸° */
    }
    </style>
    """,
    unsafe_allow_html=True
    )
    if agent_data:
        st.header("1. ì—ì´ì „íŠ¸ ë°ì´í„° ìš”ì•½")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì‚¬ìš©ì ID", agent_data.get('user_id', 'N/A'))
        with col2:
            st.metric("ì±„íŒ… ì´ë¦„", agent_data.get('chat_name', 'N/A'))
        with col3:
            st.metric("ì´ ë©”ì‹œì§€ í„´ ìˆ˜", agent_data.get('num_chat_turn', 'N/A'))

        st.subheader("ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­")
        initial_human_message = ""
        if agent_data and 'messages' in agent_data and len(agent_data['messages']) > 0:
            if agent_data['messages'][0].get('type') == 'human':
                initial_human_message = agent_data['messages'][0]['content']
        st.code(initial_human_message, language='text')

        st.header("2. ì œì•½ ì¡°ê±´ í‰ê°€ ê²°ê³¼")
        
        constraint_results = evaluate_constraints(agent_data)

        # í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”
        results_df = pd.DataFrame([constraint_results]).T
        results_df.columns = ['ì¤€ìˆ˜ ì—¬ë¶€']
        
        # ì‹œê°í™”ë¥¼ ìœ„í•œ ìƒ‰ìƒ ë§¤í•‘
        def get_status_color(value):
            if value is True:
                return ":green[âœ” ì¤€ìˆ˜]"
            elif value is False:
                return ":red[âœ– ìœ„ë°˜]"
            else:
                return ":orange[â“ ì •ë³´ ì—†ìŒ]"

        st.markdown("### ì£¼ìš” ì œì•½ ì¡°ê±´")
        st.table(results_df.map(get_status_color))

        st.markdown("### ìƒì„¸ í‰ê°€")
        st.write("ê° ì œì•½ ì¡°ê±´ì— ëŒ€í•œ ìƒì„¸ í‰ê°€ ê²°ê³¼ì…ë‹ˆë‹¤.")
        
        st.subheader("ì‚¬ìš©ì ì„ í˜¸ë„ ë¯¸ë°˜ì˜ (user_preference_reflected)")
        if constraint_results['ì‚¬ìš©ì ì„ í˜¸ë„ ë¯¸ë°˜ì˜'] is True:
            st.success("âœ… **ì¤€ìˆ˜:** `user_preference_reflected` ê°’ì´ `False`ë¡œ ì„¤ì •ë˜ì–´ ì‚¬ìš©ì ê¸°ì¡´ ì„ í˜¸ë„ê°€ ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        elif constraint_results['ì‚¬ìš©ì ì„ í˜¸ë„ ë¯¸ë°˜ì˜'] is False:
            st.error("âŒ **ìœ„ë°˜:** `user_preference_reflected` ê°’ì´ `True`ë¡œ ì„¤ì •ë˜ì–´ ì‚¬ìš©ì ê¸°ì¡´ ì„ í˜¸ë„ê°€ ë°˜ì˜ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("â“ **ì •ë³´ ì—†ìŒ:** `user_preference_reflected` ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if constraint_results['ì´ˆê¸° ìš”ì²­: ì„ í˜¸ë„ ë¯¸ë°˜ì˜ ëª…ì‹œ']:
            st.info("â„¹ï¸ ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­ì— 'Do not incorporate the user's existing preferences.' ë¬¸êµ¬ê°€ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("âš ï¸ ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­ì— 'Do not incorporate the user's existing preferences.' ë¬¸êµ¬ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        st.subheader("ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œ ê¸ˆì§€ (re_recommendation_allowed)")
        if constraint_results['ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œ ê¸ˆì§€'] is True:
            st.success("âœ… **ì¤€ìˆ˜:** `re_recommendation_allowed` ê°’ì´ `False`ë¡œ ì„¤ì •ë˜ì–´ ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œì´ í—ˆìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        elif constraint_results['ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œ ê¸ˆì§€'] is False:
            st.error("âŒ **ìœ„ë°˜:** `re_recommendation_allowed` ê°’ì´ `True`ë¡œ ì„¤ì •ë˜ì–´ ì´ì „ ì¶”ì²œê³¡ ì¬ì¶”ì²œì´ í—ˆìš©ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("â“ **ì •ë³´ ì—†ìŒ:** `re_recommendation_allowed` ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if constraint_results['ì´ˆê¸° ìš”ì²­: ì¬ì¶”ì²œ ê¸ˆì§€ ëª…ì‹œ']:
            st.info("â„¹ï¸ ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­ì— 'Do NOT allow re-recommendation of previously recommended songs.' ë¬¸êµ¬ê°€ ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("âš ï¸ ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­ì— 'Do NOT allow re-recommendation of previously recommended songs.' ë¬¸êµ¬ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        st.subheader("ìµœì¢… ì¶”ì²œ ê²°ê³¼ ë¶„ì„")
        if constraint_results['ìµœì¢… ì¶”ì²œ ì™„ë£Œ ì—¬ë¶€']:
            st.success(f"âœ… ìµœì¢… ì¶”ì²œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ì²œëœ ê³¡ ìˆ˜: {constraint_results['ì¶”ì²œëœ ê³¡ ìˆ˜']}ê³¡")
            st.markdown("### ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€")
            st.write(f"{constraint_results['ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€']}")
            if constraint_results['ì¶”ì²œ ë©”ì‹œì§€: ì´ì „ ì¶”ì²œ ë°©ì§€ ë¬¸êµ¬']:
                st.info("â„¹ï¸ ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ì— 'ì´ì „ì— ì¶”ì²œí•˜ì§€ ì•Šì•˜ë˜ ê³¡ë“¤ ì¤‘ì—ì„œ'ë¼ëŠ” ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("âš ï¸ ìµœì¢… ì¶”ì²œ ë©”ì‹œì§€ì— 'ì´ì „ì— ì¶”ì²œí•˜ì§€ ì•Šì•˜ë˜ ê³¡ë“¤ ì¤‘ì—ì„œ'ë¼ëŠ” ëª…ì‹œì ì¸ ë¬¸êµ¬ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            st.error("âŒ ìµœì¢… ì¶”ì²œì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ê±°ë‚˜ `RECOMMENDATION_COMPLETE` ë§ˆì»¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.header("3. ì›ë³¸ ì—ì´ì „íŠ¸ ë°ì´í„°")
        st.json(agent_data) # ì›ë³¸ JSON ë°ì´í„° í‘œì‹œ

    else:
        st.warning("ì—ì´ì „íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì½”ë“œì˜ `AGENT_DATA_STR` ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


def display_llm_performance(agent_data=None):
    #st.set_page_config(layout="wide")
    st.title("ğŸ“Š LLM ê¸°ë°˜ ìŒì•… ì¶”ì²œ ì„œë¹„ìŠ¤ ì„±ëŠ¥ í‰ê°€: íš¨ìœ¨ì„±, ì•ˆì •ì„±, ë ˆì´í„´ì‹œ")

    st.markdown("""
    ì´ ëŒ€ì‹œë³´ë“œëŠ” LLM ê¸°ë°˜ ìŒì•… ì¶”ì²œ ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œì¸ íš¨ìœ¨ì„± (í† í° ì‚¬ìš©ëŸ‰),
    ì•ˆì •ì„± (ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€), ê·¸ë¦¬ê³  ì‘ë‹µ ì†ë„ (ë ˆì´í„´ì‹œ)ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    """)

    st.markdown(
    """
    <style>
    div[data-testid="stMetricValue"] {
        font-size: 18px; /* ê°’(ìˆ«ì)ì˜ í°íŠ¸ í¬ê¸° */
    }
    div[data-testid="stMetricLabel"] > div {
        font-size: 14px; /* ë¼ë²¨(í…ìŠ¤íŠ¸)ì˜ í°íŠ¸ í¬ê¸° */
    }
    </style>
    """,
    unsafe_allow_html=True
    )
    if agent_data and 'messages' in agent_data:
        #df_metrics = extract_llm_metrics(agent_data)
        df_metrics = extract_single_chat_metrics(agent_data)
        print(f'\n---- df_metrics load complete ----\n')
        print(df_metrics)

        if not df_metrics.empty:
            st.header("1. ì£¼ìš” ì„±ëŠ¥ ìš”ì•½")

            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ LLM í˜¸ì¶œ ìˆ˜", len(df_metrics))
            with col2:
                st.metric("ì´ í”„ë¡¬í”„íŠ¸ í† í°", df_metrics['prompt_tokens'].sum())
            with col3:
                st.metric("ì´ ì™„ë£Œ í† í°", df_metrics['completion_tokens'].sum())
            with col4:
                st.metric("ì´ í† í°", df_metrics['total_tokens'].sum())

            col5, col6 = st.columns(2)
            with col5:
                st.metric("ì „ì²´ ì˜¤ë¥˜ ë°œìƒ ìˆ˜", df_metrics['error_count'].sum())
            with col6:
                st.metric("ëª¨ë¸ë³„ í‰ê·  í† í° ì‚¬ìš©ëŸ‰", f"{df_metrics.groupby('model_name')['total_tokens'].mean().round(2).to_dict()}", help="ëª¨ë¸ë³„ í‰ê·  í† í° ì‚¬ìš©ëŸ‰ì…ë‹ˆë‹¤.")

            st.header("2. í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„")
            st.markdown("---")
            st.subheader("ì—ì´ì „íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰")
            
            # ì—ì´ì „íŠ¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ë°” ì°¨íŠ¸
            fig_tokens_by_agent = px.bar(
                df_metrics.groupby('agent_name')[['prompt_tokens', 'completion_tokens', 'total_tokens']].sum().reset_index(),
                x='agent_name',
                y=['prompt_tokens', 'completion_tokens', 'total_tokens'],
                title='ì—ì´ì „íŠ¸ë³„ ì´ í† í° ì‚¬ìš©ëŸ‰',
                labels={'value': 'í† í° ìˆ˜', 'variable': 'í† í° ìœ í˜•', 'agent_name': 'ì—ì´ì „íŠ¸'},
                barmode='group'
            )
            st.plotly_chart(fig_tokens_by_agent, use_container_width=True)

            st.subheader("ê°œë³„ LLM í˜¸ì¶œë³„ í† í° ì‚¬ìš©ëŸ‰")
            st.dataframe(df_metrics[['agent_name', 'model_name', 'prompt_tokens', 'completion_tokens', 'total_tokens']].sort_values(by='total_tokens', ascending=False))

            st.subheader("LLM í˜¸ì¶œ ì˜ˆìƒ ë¹„ìš©")
            def load_cost_data(agent_data):
                redis_key = agent_data['redis_key']
                print(f'In Loading Cost data, redis_key: {redis_key}')
                MONGO_URI = 'mongodb://localhost:27017/'
                MONGO_DB_NAME = 'llm_monitor_db'
                MONGO_COLLECTION_NAME = 'llm_cost'
                try:
                    mongo_client = pymongo.MongoClient(MONGO_URI)
                    mongo_db_monitor = mongo_client[MONGO_DB_NAME]
                    mongo_collection_cost = mongo_db_monitor[MONGO_COLLECTION_NAME]
                    print("MongoDB ì„œë²„ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except pymongo.errors.ConnectionFailure as e:
                    print(f"MongoDB ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
                    print("MongoDB ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
                    exit()
                query = {'redis_key': redis_key}
                cost_result = list(mongo_collection_cost.find(query))
                return cost_result

            cost_data = load_cost_data(agent_data)
            if isinstance(cost_data, list):
                cost_data = cost_data[0]
            print(f'cost_data: {cost_data}')
            col7, col8, col9 = st.columns(3)
            with col7:
                st.metric("LLM ì„œë¹„ìŠ¤ íƒ€ì…", cost_data['llm_service_type'])
            with col8:
                st.metric("supervisor model", cost_data['supervisor_llm_model'])
            with col9:
                st.metric("subagent model", cost_data['subagent_llm_model'])
            
            col10, col11 = st.columns(2)
            with col10:
                st.metric("ì±„íŒ… í„´ ìˆ˜", cost_data['num_chat_turn'])
            with col11:
                st.metric("ì´ ë¹„ìš© (USD)", cost_data['total_cost']['total_cost'])
            

            def get_df_cost(cost_data):
                df_data = {'web_search_agent': cost_data['web_search_agent'],
                        'load_preference_agent': cost_data['load_preference_agent'],
                        'supervisor_agent': cost_data['supervisor_agent'],
                        'total_cost': cost_data['total_cost'],
                        }

                df_cost = pd.DataFrame(df_data)
                return df_cost
            
            st.dataframe(get_df_cost(cost_data))


            st.header("3. ì‘ë‹µ ì†ë„ (ë ˆì´í„´ì‹œ) ë¶„ì„")
            st.markdown("---")
            st.warning("âš ï¸ **ì£¼ì˜:** í˜„ì¬ ë ˆì´í„´ì‹œ ê°’ì€ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # 'latency_proxy'ë¼ëŠ” ëª©ì—… ë°ì´í„°
            st.subheader("LLM í˜¸ì¶œ ìˆœì„œë³„ ë ˆì´í„´ì‹œ (ì´ˆ)")
            fig_latency = px.line(
                df_metrics,
                x='message_id',
                y='latency', 
                title='LLM í˜¸ì¶œ ìˆœì„œë³„ ë ˆì´í„´ì‹œ',
                labels={'message_id': 'ë©”ì‹œì§€ ID', 'latency': 'ë ˆì´í„´ì‹œ (ì´ˆ)'},
                hover_data=['agent_name', 'model_name', 'total_tokens']
            )
            st.plotly_chart(fig_latency, use_container_width=True)

            st.subheader("ì—ì´ì „íŠ¸ë³„ í‰ê·  ë ˆì´í„´ì‹œ (ì´ˆ)")
            print(f'df_metrics:\n{df_metrics[['agent_name', 'latency']]}')
            avg_latency_by_agent = df_metrics.groupby('agent_name')['latency'].mean().reset_index()
            print(f'avg_latency_by_agent:\n{avg_latency_by_agent}')
            fig_avg_latency = px.bar(
                avg_latency_by_agent,
                x='agent_name',
                y='latency',
                title='ì—ì´ì „íŠ¸ë³„ í‰ê·  ë ˆì´í„´ì‹œ (í”„ë¡ì‹œ)',
                labels={'latency': 'í‰ê·  ë ˆì´í„´ì‹œ (ì´ˆ)', 'agent_name': 'ì—ì´ì „íŠ¸'}
            )
            st.plotly_chart(fig_avg_latency, use_container_width=True)


            st.header("4. ì•ˆì •ì„± (ì˜¤ë¥˜) ë¶„ì„")
            st.markdown("---")
            
            # ì—ëŸ¬ ìœ í˜•ë³„ í†µê³„
            error_counts = pd.DataFrame({
                'ì˜¤ë¥˜ ìœ í˜•': ['ê±°ë¶€ ì˜¤ë¥˜ (Refusal)', 'ìœ íš¨í•˜ì§€ ì•Šì€ íˆ´ í˜¸ì¶œ (Invalid Tool Calls)'],
                'ë°œìƒ íšŸìˆ˜': [df_metrics['refusal_error'].sum(), df_metrics['invalid_tool_calls_error'].sum()]
            })
            st.table(error_counts)

            total_errors = df_metrics['error_count'].sum()
            if total_errors > 0:
                st.error(f"âŒ **ì´ {total_errors} ê±´ì˜ LLM ê´€ë ¨ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.**")
                st.subheader("ì˜¤ë¥˜ ë°œìƒ LLM í˜¸ì¶œ ìƒì„¸")
                st.dataframe(df_metrics[df_metrics['error_count'] > 0][['message_id', 'agent_name', 'model_name', 'refusal_error', 'invalid_tool_calls_error', 'content', 'additional_kwargs']])
            else:
                st.success("âœ… **ëª¨ë“  LLM í˜¸ì¶œì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìœ¼ë©°, ì˜¤ë¥˜ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**")

            st.header("5. ì›ë³¸ ì—ì´ì „íŠ¸ ë°ì´í„° (LLM ì‘ë‹µ ë¶€ë¶„)")
            # ëª¨ë“  AI ë©”ì‹œì§€ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤Œ
            def get_llm_responses(raw_data):
                llm_responses = []
                for msg_idx, messages in enumerate(raw_data.get('messages', [])):
                    for message in messages.get('messages', []):
                        message = message.get('kwargs', {})
                        if message.get('type') == 'ai':
                            llm_responses.append(message)
                return llm_responses

            llm_responses = get_llm_responses(agent_data)

            if llm_responses:
                st.json(llm_responses)
            else:
                st.info("LLM ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        else:
            st.warning("ë¶„ì„í•  LLM í˜¸ì¶œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. `messages` í•„ë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        st.warning("ì—ì´ì „íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆê±°ë‚˜ 'messages' í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì½”ë“œì˜ `AGENT_DATA_STR` ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")



def display_llm_component_eval(agent_data=None):
    st.title("ğŸ“Š LLM ì›¹ ì„œì¹˜ì™€ ì •ë‹µ ì„±ëŠ¥ í‰ê°€")
    st.markdown(
    """
    <style>
    div[data-testid="stMetricValue"] {
        font-size: 24px; /* ê°’(ìˆ«ì)ì˜ í°íŠ¸ í¬ê¸° */
    }
    div[data-testid="stMetricLabel"] > div {
        font-size: 14px; /* ë¼ë²¨(í…ìŠ¤íŠ¸)ì˜ í°íŠ¸ í¬ê¸° */
    }
    </style>
    """,
    unsafe_allow_html=True
    )
    if agent_data:
        evaluations = get_llm_evaluations(agent_data)
        # --- LLM ì„œë¹„ìŠ¤ ì •ë³´ í‘œì‹œ ---
        st.header("âš™ï¸ LLM ì—ì´ì „íŠ¸ êµ¬ì„± ì •ë³´")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("LLM ì„œë¹„ìŠ¤ ìœ í˜•", evaluations['llm_service_type'])
            st.metric("Supervisor ëª¨ë¸", evaluations['supervisor_model'])
        with col2:
            st.metric("ê²€ìƒ‰ ë„êµ¬ ì´ë¦„", evaluations['search_tool_name'])
            st.metric("Subagent ëª¨ë¸", evaluations['subagent_model'])
        with col3:
            st.metric("ì„ë² ë”© ëª¨ë¸", evaluations['embedding_model'])
            st.metric("ì„ë² ë”© ìœ í˜•", evaluations['embedding_type'])

        st.markdown("---")

        # --- í‰ê°€ ì§€í‘œ ìš”ì•½ í‘œ ---
        st.header("ğŸ“ˆ LLM í‰ê°€ ì§€í‘œ ìš”ì•½")

        # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œë¡œ í‘œì‹œ
        eval_df = pd.DataFrame({
            'ì§€í‘œ': [
                'ì›¹ ê²€ìƒ‰ ë§¥ë½ ê´€ë ¨ì„± ì ìˆ˜',
                'ì‘ë‹µ ê´€ë ¨ì„± ì ìˆ˜',
                'ì‘ë‹µ ê·¼ê±° ì ìˆ˜'
            ],
            'ì ìˆ˜': [
                evaluations['web_search_context_relevance_score'],
                evaluations['response_relevancy_score'],
                evaluations['mean_response_groundedness_score']
            ]
        })
        # ì ìˆ˜ë¥¼ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
        eval_df['ì ìˆ˜'] = eval_df['ì ìˆ˜'].map(lambda x: f"{x:.2f}")

        st.dataframe(eval_df, hide_index=True, use_container_width=True)

        st.markdown("---")

        # --- í‰ê°€ ì§€í‘œ ì‹œê°í™” ---
        st.header("ğŸ“Š ì„¸ë¶€ í‰ê°€ ì§€í‘œ ì‹œê°í™”")

        # í‰ê°€ ì§€í‘œë¥¼ ê²Œì´ì§€ ì°¨íŠ¸ë¡œ ì‹œê°í™” (ê° ì ìˆ˜ê°€ 0ì—ì„œ 1 ì‚¬ì´ì„ì„ ê°€ì •)
        def create_gauge_chart(title, score):
            fig = go.Figure(go.Indicator(
                mode = "gauge+number",
                value = score,
                domain = {'x': [0, 1], 'y': [0, 1]},
                title = {'text': f"<span style='font-size:1.2em'>{title}</span>"},
                gauge = {
                    'axis': {'range': [None, 1], 'tickwidth': 1, 'tickcolor': "darkblue"},
                    'bar': {'color': "darkblue"},
                    'bgcolor': "white",
                    'borderwidth': 2,
                    'bordercolor': "gray",
                    'steps': [
                        {'range': [0, 0.5], 'color': 'lightgray'},
                        {'range': [0.5, 0.75], 'color': 'lightblue'},
                        {'range': [0.75, 1], 'color': 'lightgreen'}],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': score}}))
            fig.update_layout(height=250, margin=dict(l=10, r=10, t=60, b=10))
            return fig

        col_metric1, col_metric2, col_metric3 = st.columns(3)

        with col_metric1:
            st.plotly_chart(create_gauge_chart("ì›¹ ê²€ìƒ‰ ë§¥ë½ ê´€ë ¨ì„±", evaluations['web_search_context_relevance_score']), use_container_width=True)

        with col_metric2:
            st.plotly_chart(create_gauge_chart("ì‘ë‹µ ê´€ë ¨ì„±", evaluations['response_relevancy_score']), use_container_width=True)

        with col_metric3:
            st.plotly_chart(create_gauge_chart("ì‘ë‹µ ê·¼ê±° ì ìˆ˜", evaluations['mean_response_groundedness_score']), use_container_width=True)

        st.markdown("---")

        # --- ì¶”ê°€ ì •ë³´ (ì„ íƒ ì‚¬í•­) ---
        st.info("ì´ ëŒ€ì‹œë³´ë“œëŠ” LLM ì—ì´ì „íŠ¸ì˜ ì£¼ìš” í‰ê°€ ì§€í‘œë¥¼ ì‹œê°í™”í•˜ì—¬ ì‹ ì†í•œ ì„±ëŠ¥ ë¶„ì„ì„ ë•ìŠµë‹ˆë‹¤.")
        st.caption("ë°ì´í„°ëŠ” ì˜ˆì‹œì´ë©°, ì‹¤ì œ LLM í‰ê°€ ì‹œìŠ¤í…œì— ë”°ë¼ ë°ì´í„°ë¥¼ ì—°ë™í•´ì•¼ í•©ë‹ˆë‹¤.")


def disply_langsmith():
    #st.set_page_config(layout="wide")

    # LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    # í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ API í‚¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    # LANGCHAIN_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true" # Tracing V2 í™œì„±í™”

    try:
        client = Client()
    except Exception as e:
        st.error(f"LangSmith í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.info("LANGCHAIN_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    st.title("ğŸš€ LangSmith ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ")

    st.sidebar.header("í•„í„° ì„¤ì •")

    # ê¸°ê°„ í•„í„°
    time_range_options = {
        "ì§€ë‚œ 1ì‹œê°„": timedelta(hours=1),
        "ì§€ë‚œ 6ì‹œê°„": timedelta(hours=6),
        "ì§€ë‚œ 24ì‹œê°„": timedelta(hours=24),
        "ì§€ë‚œ 7ì¼": timedelta(days=7),
        "ëª¨ë‘": None
    }
    selected_time_range_label = st.sidebar.selectbox("ê¸°ê°„ ì„ íƒ", list(time_range_options.keys()))
    selected_timedelta = time_range_options[selected_time_range_label]

    # í”„ë¡œì íŠ¸ ì´ë¦„ í•„í„° (ì„ íƒ ì‚¬í•­)
    project_name = st.sidebar.text_input("í”„ë¡œì íŠ¸ ì´ë¦„ (ì„ íƒ ì‚¬í•­)", value="")

    # Trace ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    @st.cache_data(ttl=600) # 10ë¶„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
    def get_langsmith_traces(project_name=None, selected_timedelta=None):
        runs_data = []
        
        # ì‹œê°„ í•„í„° ì ìš©
        if selected_timedelta:
            start_time = datetime.now() - selected_timedelta
        else:
            start_time = None # ëª¨ë“  ê¸°ê°„
            
        try:
            # runs.list()ë¥¼ ì‚¬ìš©í•˜ì—¬ Run ê°ì²´ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            # ì´í„°ë ˆì´í„°ì´ë¯€ë¡œ ë°˜ë³µí•˜ì—¬ ëª¨ë“  Runì„ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
            for run in client.list_runs(project_name=project_name, start_time=start_time):
                runs_data.append({
                    "id": run.id,
                    "name": run.name,
                    "run_type": run.run_type,
                    "start_time": run.start_time,
                    "end_time": run.end_time,
                    "status": run.status,
                    "feedback_score": run.feedback_score,
                    "error": run.error,
                    "latency_ms": (run.end_time - run.start_time).total_seconds() * 1000 if run.start_time and run.end_time else None
                })
            return pd.DataFrame(runs_data)
        except Exception as e:
            st.error(f"LangSmith ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    df = get_langsmith_traces(project_name=project_name if project_name else None, selected_timedelta=selected_timedelta)

    if not df.empty:
        st.subheader("ğŸ“Š ì „ì²´ ëª¨ë‹ˆí„°ë§ ê°œìš”")

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ ì‹¤í–‰ ìˆ˜", len(df))
        with col2:
            successful_runs = df[df['status'] == 'completed']
            st.metric("ì„±ê³µí•œ ì‹¤í–‰", len(successful_runs))
        with col3:
            failed_runs = df[df['status'] == 'failed']
            st.metric("ì‹¤íŒ¨í•œ ì‹¤í–‰", len(failed_runs))
        with col4:
            avg_latency = df['latency_ms'].mean()
            st.metric("í‰ê·  ì§€ì—° ì‹œê°„ (ms)", f"{avg_latency:.2f}" if pd.notna(avg_latency) else "N/A")

        st.subheader("í…Œì´ë¸” í˜•ì‹ ë°ì´í„°")
        st.dataframe(df.sort_values(by="start_time", ascending=False), use_container_width=True)

        st.subheader("ğŸ“ˆ ì‹¤í–‰ íƒ€ì…ë³„ ë¶„í¬")
        run_type_counts = df['run_type'].value_counts()
        st.bar_chart(run_type_counts)

        st.subheader("ğŸš¨ ì‹¤íŒ¨í•œ ì‹¤í–‰ ëª©ë¡")
        if not failed_runs.empty:
            st.dataframe(failed_runs[['name', 'run_type', 'start_time', 'error']], use_container_width=True)
        else:
            st.info("ì‹¤íŒ¨í•œ ì‹¤í–‰ì´ ì—†ìŠµë‹ˆë‹¤.")

        st.subheader("â³ ì‹œê°„ ê²½ê³¼ì— ë”°ë¥¸ ì§€ì—° ì‹œê°„ (ìƒìœ„ 20ê°œ)")
        # ì‹¤í–‰ ì‹œê°„ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ í•„í„°ë§
        df_with_latency = df.dropna(subset=['latency_ms'])
        if not df_with_latency.empty:
            # ìƒìœ„ 20ê°œ ì‹¤í–‰ë§Œ ì‹œê°í™” (ë„ˆë¬´ ë§ìœ¼ë©´ ì°¨íŠ¸ê°€ ë³µì¡í•´ì§)
            top_20_latency = df_with_latency.sort_values(by="latency_ms", ascending=False).head(20)
            st.bar_chart(top_20_latency.set_index('name')['latency_ms'])
        else:
            st.info("ì§€ì—° ì‹œê°„ì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        st.warning("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆê±°ë‚˜ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.info("LangSmithì— í”„ë¡œì íŠ¸ê°€ ì¡´ì¬í•˜ê³  ì‹¤í–‰ ë‚´ì—­ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")