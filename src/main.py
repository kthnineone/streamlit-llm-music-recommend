import os
import json
import time
import streamlit as st
from controllers.display_chat_list import *
import services.llm_langgraph as llm_langgraph
import services.common as common
from utils.logger import setup_logger, log_structured_data
from utils.chat_redis import *
from utils.chat_mongodb import *
from utils.base import *
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import (
    BaseMessage, 
    ChatMessage, 
    SystemMessage, 
    HumanMessage, 
    AIMessage, 
    ToolMessage
)


# User_info
USER_ID = "user"
USER_NAME = "me"

#### Databases ####

# íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
current_dir = os.getcwd()
parent_dir = os.path.dirname(current_dir)
OUTPUT_FILE = "chat_history.jsonl"

# íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì • (ê° ì±„íŒ… ê¸°ë¡ì„ ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥)
CHAT_HISTORY_DIR = "../logs/chat_logs"
if not os.path.exists(CHAT_HISTORY_DIR):
    os.makedirs(CHAT_HISTORY_DIR)

redis_client = RedisClient()
mongodb_client = MongoDBClient()


#### Logging ####

logger = setup_logger(name='langgraph_logger', 
                      log_file='../logs/supervisor_stream.log')


### í˜„ì¬ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ### 

current_models_config = load_models_config()
llm_models_config = current_models_config.get('llm_models', 'Empty')
embedding_models_config = current_models_config.get('embedding_models', 'Empty')
current_models_config = current_models_config.get('current_models', 'Empty')
st.session_state['llm_models_config'] = llm_models_config
st.session_state['embedding_models_config'] = embedding_models_config
st.session_state['current_models_config'] = current_models_config

# update subagent model

#subagent_llm_model = get_subagent_llm_model(llm_models_config,
#                                            supervisor_llm_model=selected_option)
#current_models_config['subagent_llm_model'] = subagent_llm_model

#### Streamlit session_state Initilization ####

st.session_state['user_id'] = USER_ID
st.session_state['user_name'] = USER_NAME


# í˜„ì¬ ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if 'current_chat_history' not in st.session_state:
    st.session_state['current_chat_history'] = []

# ì´ì „ ì±„íŒ… ëª©ë¡ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ (ê° í•­ëª©ì€ íŒŒì¼ ì´ë¦„)
# jsonl íŒŒì¼ë“¤
previous_chats = os.listdir(CHAT_HISTORY_DIR)

# redisì—ì„œ ë¶ˆëŸ¬ì˜¨ ì´ì „ ì±„íŒ… ëª©ë¡
previous_chats = redis_client.load_previous_chats()

# mongodbì—ì„œ ë¶ˆëŸ¬ì˜¨ ì´ì „ ì±„íŒ… ëª©ë¡
previous_chats = mongodb_client.load_recent_data(USER_ID)
redis_keys_from_previous_chats = mongodb_client.get_redis_keys_from_recent_data(previous_chats)

st.session_state['previous_chats'] = previous_chats

if 'previous_chats' not in st.session_state:
    st.session_state['previous_chats'] = []

# ì„ íƒëœ ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•  ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if 'selected_chat_data' not in st.session_state:
    st.session_state['selected_chat_data'] = None

# í˜„ì¬ í‘œì‹œí•  í˜ì´ì§€ ìƒíƒœë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
if 'current_page' not in st.session_state:
    st.session_state['current_page'] = 'main'  # ì´ˆê¸° í˜ì´ì§€ëŠ” 'main'

st.set_page_config(page_title="ìŒì•… ì¶”ì²œ ì±—ë´‡ ğŸ’¬", page_icon="ğŸ’¬")

# Name Generator ìƒì„± í•¨ìˆ˜
st.session_state["name_generator"] = llm_langgraph.create_name_generator()


# st.session_state ì´ˆê¸°í™”
# ì•±ì´ ì²˜ìŒ ë¡œë“œë  ë•Œ ë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
if 'user_preference_reflected' not in st.session_state:
    st.session_state.user_preference_reflected = False # ê¸°ë³¸ê°’: ê¸°ì¡´ ì„ í˜¸ë„ ë¯¸ë°˜ì˜
if 're_recommendation_allowed' not in st.session_state:
    st.session_state.re_recommendation_allowed = True # ê¸°ë³¸ê°’: ì¬ì¶”ì²œ í—ˆìš©. No Load Preferenceë¥¼ í•˜ê¸° ìœ„í•´ì„œ.

# AI, Human, System, Tool ëª¨ë“  ë©”ì‹œì§€ ì €ì¥  
if "messages" not in st.session_state:
    st.session_state["messages"] = []

def change_user_preference_reflection(preference):
    st.session_state['user_preference_reflected'] = preference

def change_re_recommendation_allowance(allowance):
    st.session_state['re_recommendation_allowed'] = allowance

# í˜„ì¬ ìœ ì €ì˜ Chat ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸° 
#num_user_chats = load_num_chats()
num_user_chats = 6
num_user_chats = mongodb_client.count_document_by_user(USER_ID)
print(f'Initial num_user_chats: {num_user_chats}')

st.session_state['num_user_chats'] = num_user_chats

#### Chat Functions ####

# í˜„ì¬ ì±„íŒ…ì— ë©”ì‹œì§€ ì¶”ê°€ 
def add_to_current_chat(role, content):
    st.session_state['current_chat_history'].append({"role": role, "content": content})

# ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ì— ê¸°ë¡ 
def add_to_messages_log(role, messages, model=None):
    """AI, Human, System, Tool ë©”ì‹œì§€ë¥¼ ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    if model:
        st.session_state["messages"].append({"role": role, "content": messages, "model": model})
        print(f"Added to messages log: {role} (model: {model}) - {messages} ")
    else:
        st.session_state["messages"].append({"role": role, "content": messages})
        print(f"Added to messages log: {role} - {messages} ")



def filter_user_chat_content(raw_content):
    content = raw_content.replace("Allow re-recommendation of previously recommended songs.\n", "")
    content = content.replace("Do NOT allow re-recommendation of previously recommended songs.\n", "")
    content = content.replace("Incorporate the user's existing preferences.\n", "")
    content = content.replace("Do not incorporate the user's existing preferences.\n", "")
    return content

def filter_assistant_chat_content(raw_content):
    content = raw_content.replace("RECOMMENDATION_COMPLETE", "")
    #content = content.replace("temp", "")
    return content


# ì´ì „ ì±„íŒ… ê¸°ë¡ ë³´ì—¬ì£¼ê¸° 
def display_previous_chat_history_jsonl(history):
    print(f'---- history ----: \n{history}')
    for chat in history:
        for message in chat:
            if message["role"].lower() in ("user", "human"):
                chat_content = message["content"]
                chat_content = filter_user_chat_content(chat_content)
                st.chat_message("user").markdown(chat_content)
            elif message["role"].lower() in ("assistant", "ai", 'supervisor_agent'):
                chat_content = message["content"]
                if chat_content in ('Successfully transferred to web_search_agent', 'Successfully transferred to load_preference_agent'):
                    continue
                else:
                    chat_content = filter_assistant_chat_content(chat_content)
                    st.chat_message("assistant").markdown(chat_content)
            else:
                pass 
                #st.chat_message("system").markdown(chat_content)


# ì´ì „ ì±„íŒ… ê¸°ë¡ ë³´ì—¬ì£¼ê¸° 
def display_previous_chat_history_redis(history):
    print(f'---- history ----: \n{history}')
    for chat in history:
        if chat['type'].lower() in ('user', 'human'):
            chat_content = chat["content"]
            chat_content = filter_user_chat_content(chat_content)
            st.chat_message("user").markdown(chat_content)
        elif chat['type'].lower() in ('ai', 'assistant', ''):
            chat_content = chat["content"]
            if chat_content in ('Successfully transferred to web_search_agent', 'Successfully transferred to load_preference_agent', ''):
                continue
            else:
                chat_content = filter_assistant_chat_content(chat_content)
                st.chat_message("assistant").markdown(chat_content)
        else: # tool case
            pass 
            #st.chat_message("system").markdown(chat_content)

# ì´ì „ ì±„íŒ… ê¸°ë¡ ë³´ì—¬ì£¼ê¸° 
def display_previous_chat_history_mongo(history):
    print(f'---- history ----: \n{history}')
    for chat in history:
        role = chat['role'].lower()
        if role in ('user', 'human'):
            chat_content = chat["content"]
            chat_content = filter_user_chat_content(chat_content)
            st.chat_message("user").markdown(chat_content)
        elif role in ('ai', 'assistant', ''):
            chat_content = chat["content"]
            if chat_content in ('Successfully transferred to web_search_agent', 'Successfully transferred to load_preference_agent', ''):
                continue
            else:
                chat_content = filter_assistant_chat_content(chat_content)
                st.chat_message("assistant").markdown(chat_content)
        else: # tool case
            pass 
            #st.chat_message("system").markdown(chat_content)


# ì´ì „ ì±„íŒ… ê¸°ë¡ ë³´ì—¬ì£¼ê¸° 
def display_previous_chat_history(history, db_method='mongo'):
    if db_method.lower() in ('mongo', 'mongodb', 'mongo_db'):
        display_previous_chat_history_mongo(history)
    elif db_method.lower() in ('redis'):
        display_previous_chat_history_redis(history)
    else:
        display_previous_chat_history_jsonl(history)



# í˜„ì¬ ì±„íŒ… ê¸°ë¡ ë³´ì—¬ì£¼ê¸° 
def display_current_chat_history_(history):
    print(f'---- history ----: \n{history}')
    for chat in history:
        with st.chat_message(chat["role"]):
            if chat["role"] in ("user", "human"):
                chat_content = chat["content"]
                chat_content = filter_user_chat_content(chat_content)
                st.markdown(chat_content)
            elif chat["role"].lower() in ("assistant", "ai", 'supervisor_agent'):
                if chat_content in ('Successfully transferred to web_search_agent', 'Successfully transferred to load_preference_agent'):
                    continue
                else:
                    st.markdown(chat_content)
            else:
                pass 
                #st.markdown(message["content"])


def display_current_chat_history(history):
    print(f'---- history ----: \n{history}')
    for chat in history:
        if chat["role"] in ("user", "human"):
            chat_content = chat["content"]
            chat_content = filter_user_chat_content(chat_content)
            st.chat_message("user").markdown(chat_content)
        elif chat["role"].lower() in ("assistant", "ai", 'supervisor_agent'):
            chat_content = chat["content"]
            if chat_content in ('Successfully transferred to web_search_agent', 'Successfully transferred to load_preference_agent'):
                continue
            else:
                st.chat_message("assistant").markdown(chat_content)
        else:
            pass 
            #st.chat_message("system").markdown(chat["content"])



def display_main_chat_simple_chatbot():
    st.title("ë‚˜ë§Œì˜ ChatGPT ğŸ’¬")
    display_current_chat_history(st.session_state['current_chat_history'])
    #display_previous_chat_history(st.session_state['current_chat_history'])
    print(f"current model: {st.session_state['model']}")

    if "graph" not in st.session_state:
        st.session_state["graph"] = llm_langgraph.create_chat_graph(st.session_state["model"])

    # Make chat_history_id as thread_id
    # thread_id ì´ˆê¸°í™”:
    # 1. 'thread_id'ê°€ session_stateì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    # 2. 'num_user_chats'ë„ session_stateì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    #    (ë§Œì•½ num_user_chatsê°€ ì „ì²´ ì‚¬ìš©ì ì±„íŒ… ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ë©´, ì•± ì‹œì‘ ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³ ,
    #     ìƒˆë¡œìš´ ì±„íŒ… ì„¸ì…˜ì´ ì‹œì‘ë  ë•Œë§ˆë‹¤ ì¦ê°€ì‹œí‚¤ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.)
    if 'thread_id' not in st.session_state:
        if 'num_user_chats' not in st.session_state:
            st.session_state['num_user_chats'] = 0 # ì•± ì‹œì‘ ì‹œ ë˜ëŠ” ì²« ëŒ€í™” ì‹œì‘ ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        st.session_state['num_user_chats'] += 1 # ìƒˆ ëŒ€í™” ì„¸ì…˜ë§ˆë‹¤ 1 ì¦ê°€
        st.session_state['thread_id'] = st.session_state['num_user_chats'] # ìƒˆ thread_id í• ë‹¹
    
    thread_id = st.session_state['thread_id'] 
    config = RunnableConfig(
                recursion_limit=10,  # ìµœëŒ€ 10ê°œì˜ ë…¸ë“œê¹Œì§€ ë°©ë¬¸. ê·¸ ì´ìƒì€ RecursionError ë°œìƒ
                configurable={"thread_id": thread_id},  # ìŠ¤ë ˆë“œ ID ì„¤ì •
            )
    init_log = {
        'user_id': USER_ID,
        'thread_id': thread_id
        }
    log_structured_data(logger, init_log, model_name=st.session_state["model"])
    full_response = ""

    if user_input := st.chat_input():
        # chat_historyì— ì¶”ê°€ 
        add_to_current_chat("user", user_input)
        st.chat_message("user").write(user_input)
        print(f'\noriginal user_input: {user_input}')
        # ì¬ì¶”ì²œ í—ˆìš© ì—¬ë¶€ ê²°ì •
        if st.session_state.re_recommendation_allowed == True:
            user_input = "Allow re-recommendation of previously recommended songs.\n" + user_input
        else:
            user_input = "Do NOT allow re-recommendation of previously recommended songs.\n" + user_input
        # ê¸°ì¡´ ì„ í˜¸ë„ ë°˜ì˜ ì—¬ë¶€ ê²°ì •  
        if st.session_state.user_preference_reflected == True:
            user_input = "Incorporate the user's existing preferences.\n" + user_input
        else:
            user_input = "Do not incorporate the user's existing preferences.\n" + user_input

        # ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ìœ ì €ì˜ input ìˆœì„œ 
        # display_main_chatì´ ì²˜ìŒ ì‹¤í–‰ë˜ê³ 
        # ìœ ì €ì˜ ì…ë ¥ì‹œ 2ë²ˆì§¸ ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì— +1ì´ 2ë²ˆ ë˜ì–´ì„œ 2ê°€ ë‚˜ì˜¨ë‹¤. 
        if 'current_num_chat_turn' not in st.session_state:
            st.session_state['current_num_chat_turn'] = 1
        else:
            st.session_state['current_num_chat_turn'] += 1
        print(f'\nmodified user_input: {user_input}')
        print(f'\nchat turn: {st.session_state['current_num_chat_turn']}, thread_id: {thread_id}')
        # ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        add_to_messages_log("user", user_input)

        chat_history = st.session_state['current_chat_history']
        num_previous_chat_history = len(chat_history) - 1
        # modified user_inputìœ¼ë¡œ ë³€ê²½ 
        chat_history[-1] = {"role": 'user', "content": user_input}
        
        # AI assistant ì‘ë‹µ ìƒì„± ì„¹ì…˜ 
        with st.chat_message("assistant"):
            chat_container = st.empty()
            status_container = st.empty()  # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ

            # LangGraph ì‹¤í–‰ 
            status_container.markdown('ê²°ê³¼ë¥¼ ìƒì„±ì¤‘...')
            for output in st.session_state["graph"].stream({"messages": chat_history},
                                                                       config=config):
                print(f'\noutput type: {type(output)}')
                output['num_chat_turn'] = st.session_state['current_num_chat_turn']
                print(f'\noutput:\n {output}')
                # ë¡œê·¸ì— ì¶œë ¥ ë‚´ìš© ê¸°ë¡
                log_structured_data(logger, output, model_name=st.session_state["model"])
                if "chatbot" in output: # output.keys()ëŠ” ëª¨ë“  í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë·° ê°ì²´ë¥¼ ë§Œë“¤ê¸°ì— ì˜¤ë²„í—¤ë“œ ë°œìƒ 
                    #print(f'\noutput of chatbot type: {type(output['chatbot'])}')
                    print(f'\n----- output of chatbot -----: {output["chatbot"]}')
                    if isinstance(output["chatbot"], dict) and "messages" in output["chatbot"]:
                        last_message = output["chatbot"]["messages"][-1]
                        content = last_message.content
                        meta_data = last_message.response_metadata  # ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ ìˆë‹¤ë©´ ê°€ì ¸ì˜¤ê¸°
                        model_name = meta_data.get('model_name', None)
                        full_response += content
                        chat_container.markdown(full_response + "â–Œ") # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ìœ„í•œ ì»¤ì„œ í‘œì‹œ
                        # ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€
                        add_to_messages_log("assistant", content, model_name)
                        
                        # session stateì˜ 'current_chat_history'ì— ì¶”ê°€ 
                        #add_to_current_chat('assistant', content)
                elif "tools" in output:
                    #print(f'\noutput of chatbot type: {type(output['chatbot'])}')
                    print(f'\n----- output of tools -----: {output["tools"]}')
                    status_container.markdown('ì›¹ ê²€ìƒ‰ ì¤‘...')
                    content = output["tools"]['messages'][-1].content
                    # ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€ 
                    add_to_messages_log("tool", content)
                    continue
                elif "END" in output:
                    st.markdown("ê²°ê³¼ ìƒì„± ì™„ë£Œ")
                    #st.session_state["graph"].reset()  # ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”
                    break

                # ìƒíƒœ ë©”ì‹œì§€ ì œê±°
                status_container.empty()

            # ìµœì¢… ì‘ë‹µ ê²°ê³¼ í‘œì‹œ
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        # chat_historyì— ì¶”ê°€
        add_to_current_chat("assistant", full_response)
        print(f'\nchat_history: \n{chat_history}')
        
        # Assistant ì„¸ì…˜ ë 
        # ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ ì €ì¥
        save_messages_log(messages_to_save=st.session_state['messages'])
        # Redisì— ì €ì¥  
        output_dict = dict(output)
        # ì±„íŒ… ì´ë¦„ ìƒì„± 
        if 'chat_name' in st.session_state:
            chat_name = st.session_state["chat_name"]
        else:
            name_generator = st.session_state["name_generator"]
            chat_history_for_name = [user_input, full_response]
            chat_name = name_generator.invoke({"messages": chat_history_for_name})
            st.session_state["chat_name"] = chat_name
        # Toolì„ í¬í•¨í•œ ì „ì²´ ë©”ì‹œì§€, ëª¨ë‹ˆí„°ë§ìš© 
        raw_messages_json = convert_to_json_for_raw(USER_ID,
                                                USER_NAME,
                                                thread_id,
                                                chat_name,
                                                st.session_state['current_num_chat_turn'],
                                                st.session_state['user_preference_reflected'],
                                                st.session_state['re_recommendation_allowed'],
                                                output_dict)
        # metadata ì¶”ê°€ 
        raw_messages_json.update(current_models_config)
        redis_key = f'{USER_ID}:{thread_id}'
        # Redisì— ì €ì¥ 
        redis_client.save_into_redis(raw_messages_json, redis_key)
        raw_messages_json['redis_key'] = redis_key
        # MongoDBì— ì €ì¥ 
        mongodb_client.insert_update_raw_messages(raw_messages_json)
        # ìœ ì €ì—ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ëŒ€í™” ë‚´ì—­ 
        messages_for_user_json = {'user_id': USER_ID,
                                'user_name': USER_NAME,
                                'thread_id': thread_id,
                                'chat_name': chat_name,
                                'redis_key': redis_key,
                                'messages': st.session_state['current_chat_history']}
        # MongoDBì— ì €ì¥ 
        mongodb_client.insert_update_messages_for_user(messages_for_user_json)





def display_main_chat():
    st.title("ë‚˜ë§Œì˜ ChatGPT ğŸ’¬")
    display_current_chat_history(st.session_state['current_chat_history'])
    #display_previous_chat_history(st.session_state['current_chat_history'])
    print(f"Display Main Chat, current model: {st.session_state['model']}")

    if "graph" not in st.session_state:
        st.session_state["graph"] = llm_langgraph.create_chat_graph(st.session_state["model"])

    # Make chat_history_id as thread_id
    # thread_id ì´ˆê¸°í™”:
    # 1. 'thread_id'ê°€ session_stateì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    # 2. 'num_user_chats'ë„ session_stateì— ì—†ìœ¼ë©´ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    #    (ë§Œì•½ num_user_chatsê°€ ì „ì²´ ì‚¬ìš©ì ì±„íŒ… ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ë©´, ì•± ì‹œì‘ ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ê³ ,
    #     ìƒˆë¡œìš´ ì±„íŒ… ì„¸ì…˜ì´ ì‹œì‘ë  ë•Œë§ˆë‹¤ ì¦ê°€ì‹œí‚¤ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.)
    if 'thread_id' not in st.session_state:
        if 'num_user_chats' not in st.session_state:
            st.session_state['num_user_chats'] = 0 # ì•± ì‹œì‘ ì‹œ ë˜ëŠ” ì²« ëŒ€í™” ì‹œì‘ ì‹œ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        st.session_state['num_user_chats'] += 1 # ìƒˆ ëŒ€í™” ì„¸ì…˜ë§ˆë‹¤ 1 ì¦ê°€
        st.session_state['thread_id'] = st.session_state['num_user_chats']# ìƒˆ thread_id í• ë‹¹
    
    thread_id = st.session_state['thread_id'] 
    config = RunnableConfig(
                recursion_limit=10,  # ìµœëŒ€ 10ê°œì˜ ë…¸ë“œê¹Œì§€ ë°©ë¬¸. ê·¸ ì´ìƒì€ RecursionError ë°œìƒ
                configurable={"thread_id": thread_id},  # ìŠ¤ë ˆë“œ ID ì„¤ì •
            )
    init_log = {
        'user_id': USER_ID,
        'thread_id': thread_id
        }
    log_structured_data(logger, init_log, model_name=st.session_state["model"])
    full_response = ""

    if user_input := st.chat_input():
        print(f'If user_input, Current Models Config: {st.session_state['current_models_config']}')
        # chat_historyì— ì¶”ê°€ 
        add_to_current_chat("user", user_input)
        st.chat_message("user").write(user_input)
        print(f'\noriginal user_input: {user_input}')
        # ì¬ì¶”ì²œ í—ˆìš© ì—¬ë¶€ ê²°ì •
        if st.session_state.re_recommendation_allowed == True:
            user_input = "Allow re-recommendation of previously recommended songs.\n" + user_input
        else:
            user_input = "Do NOT allow re-recommendation of previously recommended songs.\n" + user_input
        # ê¸°ì¡´ ì„ í˜¸ë„ ë°˜ì˜ ì—¬ë¶€ ê²°ì •  
        if st.session_state.user_preference_reflected == True:
            user_input = "Incorporate the user's existing preferences.\n" + user_input
        else:
            user_input = "Do not incorporate the user's existing preferences.\n" + user_input
        # ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ìœ ì €ì˜ input ìˆœì„œ 
        # display_main_chatì´ ì²˜ìŒ ì‹¤í–‰ë˜ê³ 
        # ìœ ì €ì˜ ì…ë ¥ì‹œ 2ë²ˆì§¸ ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì— +1ì´ 2ë²ˆ ë˜ì–´ì„œ 2ê°€ ë‚˜ì˜¨ë‹¤. 
        if 'current_num_chat_turn' not in st.session_state:
            st.session_state['current_num_chat_turn'] = 1
        else:
            st.session_state['current_num_chat_turn'] += 1
        print(f'\nmodified user_input: {user_input}')
        print(f'\nchat turn: {st.session_state['current_num_chat_turn']}, thread_id: {thread_id}')
        # ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        add_to_messages_log("user", user_input)

        chat_history = st.session_state['current_chat_history']
        num_previous_chat_history = len(chat_history) - 1
        # modified user_inputìœ¼ë¡œ ë³€ê²½ 
        chat_history[-1] = {"role": 'user', "content": user_input}
        
        # AI assistant ì‘ë‹µ ìƒì„± ì„¹ì…˜ 
        with st.chat_message("assistant"):
            chat_container = st.empty()
            status_container = st.empty()  # ìƒíƒœ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ

            # LangGraph ì‹¤í–‰ 
            status_container.markdown('ê²°ê³¼ë¥¼ ìƒì„±ì¤‘...')
            for output in st.session_state["graph"].stream({"messages": chat_history},
                                                                       config=config):
                print(f'\noutput type: {type(output)}')
                output['num_chat_turn'] = st.session_state['current_num_chat_turn']
                print(f'\noutput:\n {output}')
                # ë¡œê·¸ì— ì¶œë ¥ ë‚´ìš© ê¸°ë¡
                log_structured_data(logger, output, model_name=st.session_state["model"])
                # supervisor_agent case 
                if "supervisor_agent" in output:
                    messages = output["supervisor_agent"]["messages"]
                    if messages:
                        last_message = messages[-1]
                        # ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€
                        print(f'\n----- supervisor agent step -----')
                        print(f'----- last_message -----: {last_message}')
                        content = last_message.content
                        meta_data = last_message.response_metadata  # ë©”íƒ€ë°ì´í„° ì •ë³´ê°€ ìˆë‹¤ë©´ ê°€ì ¸ì˜¤ê¸°
                        model_name = meta_data.get('model_name', None)
                        add_to_messages_log("supervisor_agent", content, model_name)

                        # session stateì˜ 'current_chat_history'ì— ì¶”ê°€ 
                        #add_to_current_chat('assistant', content)
                        
                        # Supervisorê°€ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²½ìš° (create_handoff_toolì— ì˜í•´ Commandê°€ ë°˜í™˜ë¨)
                        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                            tool_name = last_message.tool_calls[0]["name"] # 'name' ì†ì„± ì‚¬ìš©
                            status_container.markdown(f'{tool_name} í˜¸ì¶œ ì¤‘...')
                            continue # ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ìœ¼ë¡œ ë„˜ì–´ê°
                        
                        # Supervisorê°€ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê²½ìš°
                        content = last_message.content
                        if "RECOMMENDATION_COMPLETE" in content:
                            full_response += content.replace("RECOMMENDATION_COMPLETE", "").strip()
                            chat_container.markdown(full_response)
                            status_container.markdown("ì¶”ì²œ ê²°ê³¼ ìƒì„± ì™„ë£Œ")
                            break # 'RECOMMENDATION_COMPLETE' í‚¤ì›Œë“œ í™•ì¸ í›„ ë£¨í”„ ì¢…ë£Œ
                        else:
                            # ìŠ¤íŠ¸ë¦¬ë° ì¤‘ê°„ ì‘ë‹µ
                            # gemini ëª¨ë¸ì˜ ê²½ìš° íŒ¨ìŠ¤ 
                            if 'gemini' in st.session_state["model"].lower():
                                continue
                            content = content.replace("Successfully transferred to web_search_agent", "").strip()
                            content = content.replace("Successfully transferred to load_preference_agent", "").strip()
                            full_response += content
                            #chat_container.markdown(full_response + "â–Œ")
                            status_container.markdown('ë‹¤ìŒ ìŠ¤í…ì„ ê²€í†  ì¤‘...')
                            continue # ë‹¤ìŒ ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ìœ¼ë¡œ ë„˜ì–´ê°
                # Agent ìƒíƒœ ë©”ì‹œì§€ ì²˜ë¦¬ 
                elif "web_search_agent" in output:
                    print(f'\n----- output of web_search_agent -----:\n{output["web_search_agent"]['messages']}')
                    status_container.markdown('ì›¹ ê²€ìƒ‰ ì¤‘...')
                    # ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€ 
                    content = output["web_search_agent"]["messages"][-1].content
                    add_to_messages_log("web_search_agent", content)
                    # Gemini ëª¨ë¸ì˜ ê²½ìš° web_search_agentì˜ ë©”ì‹œì§€ë¥¼ full_responseì— ì¶”ê°€í•´ì•¼ í•¨ 
                    if 'gemini' in st.session_state["model"].lower():
                        full_response += '\n' + content
                    continue
                elif "load_preference_agent" in output:
                    print(f'\n-----output of load_preference_agent -----:\n {output["load_preference_agent"]['messages']}')
                    status_container.markdown('ê¸°ì¡´ ìœ ì €ì˜ ì„ í˜¸ ìŒì•… ì •ë³´ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...')
                    # ë©”ì‹œì§€ ë¡œê·¸ì— ì¶”ê°€ 
                    content = output["load_preference_agent"]["messages"][-1].content
                    add_to_messages_log("load_preference_agent", content)
                    continue
                elif "END" in output:
                    st.markdown("ê²°ê³¼ ìƒì„± ì™„ë£Œ")
                    #st.session_state["graph"].reset()  # ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”
                    break

            # ìƒíƒœ ë©”ì‹œì§€ ì œê±°
            status_container.empty()

            # ìµœì¢… ì‘ë‹µ ê²°ê³¼ í‘œì‹œ
            if not full_response and "supervisor_agent" in output: # 'RECOMMENDATION_COMPLETE'ë¡œ ë°”ë¡œ ëë‚˜ëŠ” ê²½ìš° ëŒ€ë¹„
                final_messages = output["supervisor_agent"]["messages"]
                if final_messages:
                    final_content = final_messages[-1].content.replace("RECOMMENDATION_COMPLETE", "").strip()
                    full_response = final_content
                    chat_container.markdown(full_response)
            elif full_response:
                 chat_container.markdown(full_response)
            #st.session_state.messages.append({"role": "assistant", "content": full_response})

        # chat_historyì— ì¶”ê°€
        add_to_current_chat("assistant", full_response)
        print(f'\nchat_history: \n{chat_history}')
        
        # Assistant ì„¸ì…˜ ë 
        # ì „ì²´ ë©”ì‹œì§€ ë¡œê·¸ ì €ì¥
        save_messages_log(messages_to_save=st.session_state['messages'])
        # Outputë¥¼ ë”•ì…”ë„ˆë¦¬ í™”  
        output_dict = dict(output)
        # ì±„íŒ… ì´ë¦„ ìƒì„± 
        if 'chat_name' in st.session_state:
            if st.session_state["chat_name"] != '':
                chat_name = st.session_state["chat_name"]
            else:
                name_generator = st.session_state["name_generator"]
                chat_history_for_name = [user_input, full_response]
                chat_name = name_generator.invoke({"messages": chat_history_for_name})
                st.session_state["chat_name"] = chat_name
        else:
            name_generator = st.session_state["name_generator"]
            chat_history_for_name = [user_input, full_response]
            chat_name = name_generator.invoke({"messages": chat_history_for_name})
            st.session_state["chat_name"] = chat_name
        # Toolì„ í¬í•¨í•œ ì „ì²´ ë©”ì‹œì§€, ëª¨ë‹ˆí„°ë§ìš©
        # ì´ì „ì˜ chat_history íŒŒíŠ¸ ì œê±°ë¥¼ ìœ„í•´ì„œ num_previous_chat_history ì¶”ê°€
        raw_messages_json = convert_to_json_for_raw(USER_ID,
                                                USER_NAME,
                                                thread_id,
                                                chat_name,
                                                st.session_state['current_num_chat_turn'],
                                                st.session_state['user_preference_reflected'],
                                                st.session_state['re_recommendation_allowed'],
                                                output_dict,
                                                num_previous_chat_history)
        # metadata ì¶”ê°€ 
        raw_messages_json.update(st.session_state['current_models_config'])
        print(f'In the Saving Process, Current Models Config: {st.session_state['current_models_config']}')
        redis_key = f'{USER_ID}:{thread_id}'
        # Redisì— ì €ì¥ 
        redis_client.save_into_redis(raw_messages_json, redis_key)
        raw_messages_json['redis_key'] = redis_key
        # MongoDBì— ì €ì¥ 
        mongodb_client.insert_update_raw_messages(raw_messages_json)
        # ìœ ì €ì—ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•œ ëŒ€í™” ë‚´ì—­ 
        messages_for_user_json = {'user_id': USER_ID,
                                'user_name': USER_NAME,
                                'thread_id': thread_id,
                                'num_chat_turn': st.session_state['current_num_chat_turn'],
                                'chat_name': chat_name,
                                'redis_key': redis_key,
                                'messages': st.session_state['current_chat_history']}
        # MongoDBì— ì €ì¥ 
        mongodb_client.insert_update_messages_for_user(messages_for_user_json)




def display_previous_chat_jsonl(filename):
    st.title("ì´ì „ ì±„íŒ…")
    history = load_chat_history(filename)
    display_previous_chat_history(history)
    if st.button("ëŒì•„ê°€ê¸°"):
        st.session_state['current_page'] = 'main'
        st.session_state['selected_chat_data'] = None
        #st.rerun()

def display_previous_chat_redis(previous_messages):
    st.title("ì´ì „ ì±„íŒ…")
    history = previous_messages
    display_previous_chat_history(history)
    if st.button("ëŒì•„ê°€ê¸°"):
        st.session_state['current_page'] = 'main'
        st.session_state['selected_chat_data'] = None
        #st.rerun()

def display_previous_chat(previous_chat_data, is_redis=True):
    if is_redis:
        display_previous_chat_redis(previous_chat_data)
    else:
        display_previous_chat_jsonl(previous_chat_data)

# ìƒˆë¡œìš´ ì±„íŒ… ì‹œì‘ 
def start_new_chat():
    '''
    if st.session_state['current_chat_history']:
        # í˜„ì¬ ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ê³  ì´ì „ ì±„íŒ… ëª©ë¡ì— ì¶”ê°€
        filename = f"chat_{len(st.session_state['previous_chats']) + 1}"
        print(f'\ntype of current_chat_history: {type(st.session_state["current_chat_history"])}')
        print(f'current_chat_history: {st.session_state["current_chat_history"]}\n')
        name_generator = st.session_state["name_generator"]
        name = name_generator.invoke({"messages": st.session_state['current_chat_history']})
        save_chat_history_to_jsonl(st.session_state['current_chat_history'], name)
        st.session_state['previous_chats'].append(name)
        st.session_state['current_chat_history'] = []
        st.session_state['selected_chat_data'] = None
        st.session_state['current_page'] = 'main'
        st.rerun()
    '''
    if 'chat_name' in st.session_state:
        st.session_state['current_chat_history'] = []
        st.session_state['selected_chat_data'] = None
        st.session_state['current_page'] = 'main'
        st.session_state['num_user_chats'] += 1 # ìƒˆ ëŒ€í™” ì„¸ì…˜ë§ˆë‹¤ 1 ì¦ê°€
        st.session_state['thread_id'] += 1 # ìƒˆ ëŒ€í™” ì„¸ì…˜ë§ˆë‹¤ 1 ì¦ê°€
        st.session_state['current_num_chat_turn'] = 0 # ìƒˆ ëŒ€í™”ë§ˆë‹¤ ì´ˆê¸°í™” 
        st.session_state["chat_name"] = ''

        #st.rerun()
    else:
        pass



with st.sidebar:
    # ëª¨ë¸ ì„ íƒ UI  
    st.title("ëª¨ë¸ ì„ íƒ")

    ## ì˜µì…˜ ì •ì˜
    model_options = common.availabe_models
    print(f'Sidebar, model_options: {model_options}')
 
    ## ì„ íƒ ìƒì ìƒì„±
    selected_option = st.selectbox('ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:', model_options)

    # ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if st.session_state.get("model") != selected_option:
        print('\n---- In Streamlit sidebar session ----')
        print(f'model changed from {st.session_state.get("model")} to {selected_option}')
        st.session_state["model"] = selected_option
        # ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆë‹¤ë©´ ê·¸ë˜í”„ë¥¼ ì¬ìƒì„±
        if "graph" in st.session_state:
            del st.session_state["graph"]  # ê¸°ì¡´ ê·¸ë˜í”„ ì‚­ì œ
        st.session_state["graph"] = llm_langgraph.create_chat_graph(selected_option)
        st.toast(f"ëª¨ë¸ì´ {selected_option}ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # metadataì¸ current_models ë”•ì…”ë„ˆë¦¬ì˜ ê°’ë„ ë³€ê²½
        current_models_config['supervisor_llm_model'] = selected_option
        subagent_llm_model = get_subagent_llm_model(llm_models_config,
                                                    supervisor_llm_model=selected_option)
        current_models_config['subagent_llm_model'] = subagent_llm_model
        print(f'\nSelected Supervisor Agent Model: {selected_option}')
        print(f'Selected Sub-Agent Model: {subagent_llm_model}')
        st.session_state['current_models_config'] = current_models_config
        print(f'In sidebar, Current Models Config: {st.session_state['current_models_config']}')

    st.markdown("---")  # ìˆ˜í‰ì„  ì¶”ê°€

    # ê¸°ì¡´ ì„ í˜¸ ë°ì´í„° ë°˜ì˜ ì—¬ë¶€  
    st.title("ê¸°ì¡´ ì„ í˜¸ ë°ì´í„°")
    st.sidebar.header("ì´ì „ ì„ í˜¸ ë°ì´í„° ë°˜ì˜ ì—¬ë¶€")
    pref_reflect, pref_not_reflect = st.columns(2)  # ë‘ ê°œì˜ ì»¬ëŸ¼ ìƒì„±
    pref_reflect.button('ê¸°ì¡´ ì„ í˜¸ ë°˜ì˜', on_click=change_user_preference_reflection, args=[True])
    pref_not_reflect.button('ê¸°ì¡´ ì„ í˜¸ ë¯¸ë°˜ì˜', on_click=change_user_preference_reflection, args=[False])
    st.write(f"**í˜„ì¬ ìƒíƒœ:** {'âœ… ë°˜ì˜ë¨' if st.session_state.user_preference_reflected else 'âŒ ë¯¸ë°˜ì˜'}")
    st.sidebar.header("ì´ì „ ì„ í˜¸ ë…¸ë˜ ì¬ì¶”ì²œ ì—¬ë¶€")
    re_rec, no_re_rec = st.columns(2)  # ë‘ ê°œì˜ ì»¬ëŸ¼ ìƒì„±
    re_rec.button("ì¬ì¶”ì²œ í—ˆìš©", use_container_width=True, on_click=change_re_recommendation_allowance, args=[True])
    no_re_rec.button("ì¬ì¶”ì²œ ë¹„í—ˆìš©", use_container_width=True, on_click=change_re_recommendation_allowance, args=[False])
    st.write(f"**í˜„ì¬ ìƒíƒœ:** {'ğŸ”„ ì¬ì¶”ì²œ í—ˆìš©' if st.session_state.re_recommendation_allowed else 'âŒ ì¬ì¶”ì²œ ë¹„í—ˆìš©'}")


    st.markdown("---") # êµ¬ë¶„ì„ 

    col1, col2 = st.columns([1, 1])  # ì»¬ëŸ¼ ë„ˆë¹„ ë¹„ìœ¨ ì¡°ì • ê°€ëŠ¥
    with col1:
        st.title('ì±„íŒ… ê´€ë¦¬')
    with col2:
        st.markdown(
        """
        <style>
            div[data-testid="column"]:nth-child(2) {
                display: flex;
                align-items: center;
                justify-content: flex-end; /* ì˜¤ë¥¸ìª½ ì •ë ¬ ìœ ì§€ */
                height: 100%; /* ì»¬ëŸ¼ ë†’ì´ë¥¼ ë¶€ëª¨ ìš”ì†Œì— ë§ì¶¤ */
            }
        </style>
        """,
        unsafe_allow_html=True,
        )
        st.button("ìƒˆë¡œìš´ ì±„íŒ…", on_click=start_new_chat)

    st.sidebar.header("ì´ì „ ì±„íŒ… ëª©ë¡")
    if st.session_state['previous_chats']:
        disply_previous_chat_list_in_sidebar()





# í˜„ì¬ í˜ì´ì§€ ìƒíƒœì— ë”°ë¼ ì½˜í…ì¸  í‘œì‹œ
if st.session_state['current_page'] == 'main':
    display_main_chat()
elif st.session_state['current_page'] == 'previous_chat' and st.session_state['selected_chat_data']:
    display_previous_chat(st.session_state['selected_chat_data'])